{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: selenium in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.35.0)\n",
            "Requirement already satisfied: playwright in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.55.0)\n",
            "Requirement already satisfied: lxml in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.1)\n",
            "Requirement already satisfied: html5lib in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: trio~=0.30.0 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: pyee<14,>=13 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from playwright) (13.0.0)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from playwright) (3.2.4)\n",
            "Requirement already satisfied: six>=1.9 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from html5lib) (1.17.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from html5lib) (0.5.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.22)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\diya shah\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install requests beautifulsoup4 pandas selenium playwright lxml html5lib\n",
        "!playwright install chromium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from playwright.sync_api import sync_playwright\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q1: Books to Scrape Website\n",
        "\n",
        "Scraping all books from https://books.toscrape.com/ with pagination handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting to scrape books...\n",
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Scraping page 21...\n",
            "Scraping page 22...\n",
            "Scraping page 23...\n",
            "Scraping page 24...\n",
            "Scraping page 25...\n",
            "Scraping page 26...\n",
            "Scraping page 27...\n",
            "Scraping page 28...\n",
            "Scraping page 29...\n",
            "Scraping page 30...\n",
            "Scraping page 31...\n",
            "Scraping page 32...\n",
            "Scraping page 33...\n",
            "Scraping page 34...\n",
            "Scraping page 35...\n",
            "Scraping page 36...\n",
            "Scraping page 37...\n",
            "Scraping page 38...\n",
            "Scraping page 39...\n",
            "Scraping page 40...\n",
            "Scraping page 41...\n",
            "Scraping page 42...\n",
            "Scraping page 43...\n",
            "Scraping page 44...\n",
            "Scraping page 45...\n",
            "Scraping page 46...\n",
            "Scraping page 47...\n",
            "Scraping page 48...\n",
            "Scraping page 49...\n",
            "Scraping page 50...\n",
            "Scraping page 51...\n",
            "Error scraping page 51: 404 Client Error: Not Found for url: https://books.toscrape.com/catalogue/page-51.html\n",
            "\n",
            "Scraping completed! Total books scraped: 1000\n",
            "Data saved to 'books.csv'\n",
            "\n",
            "First 5 books:\n",
            "                                   Title   Price Availability Star Rating\n",
            "0                   A Light in the Attic  £51.77     In stock       Three\n",
            "1                     Tipping the Velvet  £53.74     In stock         One\n",
            "2                             Soumission  £50.10     In stock         One\n",
            "3                          Sharp Objects  £47.82     In stock        Four\n",
            "4  Sapiens: A Brief History of Humankind  £54.23     In stock        Five\n"
          ]
        }
      ],
      "source": [
        "def scrape_books():\n",
        "    \"\"\"\n",
        "    Scrape all books from books.toscrape.com with pagination\n",
        "    \"\"\"\n",
        "    base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
        "    all_books = []\n",
        "    page = 1\n",
        "    \n",
        "    print(\"Starting to scrape books...\")\n",
        "    \n",
        "    while True:\n",
        "        url = base_url.format(page)\n",
        "        print(f\"Scraping page {page}...\")\n",
        "        \n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            \n",
        "            # Find all book containers\n",
        "            books = soup.find_all('article', class_='product_pod')\n",
        "            \n",
        "            if not books:\n",
        "                print(f\"No books found on page {page}. Scraping complete.\")\n",
        "                break\n",
        "            \n",
        "            for book in books:\n",
        "                # Extract title\n",
        "                title_tag = book.find('h3').find('a')\n",
        "                title = title_tag['title']\n",
        "                \n",
        "                # Extract price\n",
        "                price_tag = book.find('p', class_='price_color')\n",
        "                price = price_tag.text.strip()\n",
        "                \n",
        "                # Extract availability\n",
        "                availability_tag = book.find('p', class_='instock availability')\n",
        "                availability = availability_tag.text.strip()\n",
        "                \n",
        "                # Extract star rating\n",
        "                star_tag = book.find('p', class_=lambda x: x and 'star-rating' in x)\n",
        "                star_rating = 'Unknown'\n",
        "                if star_tag:\n",
        "                    classes = star_tag.get('class', [])\n",
        "                    for cls in classes:\n",
        "                        if cls in ['One', 'Two', 'Three', 'Four', 'Five']:\n",
        "                            star_rating = cls\n",
        "                            break\n",
        "                \n",
        "                book_data = {\n",
        "                    'Title': title,\n",
        "                    'Price': price,\n",
        "                    'Availability': availability,\n",
        "                    'Star Rating': star_rating\n",
        "                }\n",
        "                all_books.append(book_data)\n",
        "            \n",
        "            page += 1\n",
        "            time.sleep(1)  # Be respectful to the server\n",
        "            \n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error scraping page {page}: {e}\")\n",
        "            break\n",
        "    \n",
        "    # Create DataFrame and save to CSV\n",
        "    df_books = pd.DataFrame(all_books)\n",
        "    df_books.to_csv('books.csv', index=False)\n",
        "    \n",
        "    print(f\"\\nScraping completed! Total books scraped: {len(all_books)}\")\n",
        "    print(\"Data saved to 'books.csv'\")\n",
        "    print(\"\\nFirst 5 books:\")\n",
        "    print(df_books.head())\n",
        "    \n",
        "    return df_books\n",
        "\n",
        "# Run the scraping\n",
        "books_df = scrape_books()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q2: IMDB Top 250 Movies\n",
        "\n",
        "Scraping IMDB Top 250 movies using Selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting IMDB Top 250 scraping with Selenium...\n"
          ]
        }
      ],
      "source": [
        "def scrape_imdb_top250_selenium():\n",
        "    \"\"\"\n",
        "    Scrape IMDB Top 250 movies using Selenium\n",
        "    \"\"\"\n",
        "    print(\"Starting IMDB Top 250 scraping with Selenium...\")\n",
        "    \n",
        "    # Setup Chrome options\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')  # Run in background\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
        "    \n",
        "    try:\n",
        "        # Initialize the webdriver\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        driver.get('https://www.imdb.com/chart/top/')\n",
        "        \n",
        "        # Wait for page to load\n",
        "        wait = WebDriverWait(driver, 10)\n",
        "        \n",
        "        # Find all movie containers\n",
        "        movies_list = []\n",
        "        \n",
        "        # Try different selectors for movie items\n",
        "        try:\n",
        "            movie_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.titleColumn')))\n",
        "        except:\n",
        "            movie_elements = driver.find_elements(By.CSS_SELECTOR, 'li[data-testid=\"imdb-chart-row\"]')\n",
        "        \n",
        "        if not movie_elements:\n",
        "            movie_elements = driver.find_elements(By.CSS_SELECTOR, '.cli-item')\n",
        "        \n",
        "        print(f\"Found {len(movie_elements)} movie elements\")\n",
        "        \n",
        "        for i, movie in enumerate(movie_elements[:250], 1):\n",
        "            try:\n",
        "                # Extract rank\n",
        "                rank = i\n",
        "                \n",
        "                # Extract title and year\n",
        "                title_element = movie.find_element(By.CSS_SELECTOR, 'a')\n",
        "                full_title = title_element.text\n",
        "                \n",
        "                # Extract year from title or separate element\n",
        "                year_match = re.search(r'\\((\\d{4})\\)', full_title)\n",
        "                if year_match:\n",
        "                    year = year_match.group(1)\n",
        "                    title = full_title.replace(f'({year})', '').strip()\n",
        "                else:\n",
        "                    title = full_title\n",
        "                    year = 'Unknown'\n",
        "                \n",
        "                # Extract rating\n",
        "                try:\n",
        "                    rating_element = movie.find_element(By.CSS_SELECTOR, '.ratingColumn strong')\n",
        "                    rating = rating_element.text\n",
        "                except:\n",
        "                    try:\n",
        "                        rating_element = movie.find_element(By.CSS_SELECTOR, '[data-testid=\"imdb-rating\"] span')\n",
        "                        rating = rating_element.text\n",
        "                    except:\n",
        "                        rating = 'Unknown'\n",
        "                \n",
        "                movie_data = {\n",
        "                    'Rank': rank,\n",
        "                    'Movie Title': title,\n",
        "                    'Year of Release': year,\n",
        "                    'IMDB Rating': rating\n",
        "                }\n",
        "                movies_list.append(movie_data)\n",
        "                \n",
        "                if i % 50 == 0:\n",
        "                    print(f\"Processed {i} movies...\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing movie {i}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        driver.quit()\n",
        "        \n",
        "        # Create DataFrame and save to CSV\n",
        "        df_movies = pd.DataFrame(movies_list)\n",
        "        df_movies.to_csv('imdb_top250.csv', index=False)\n",
        "        \n",
        "        print(f\"\\nScraping completed! Total movies scraped: {len(movies_list)}\")\n",
        "        print(\"Data saved to 'imdb_top250.csv'\")\n",
        "        print(\"\\nFirst 10 movies:\")\n",
        "        print(df_movies.head(10))\n",
        "        \n",
        "        return df_movies\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error during scraping: {e}\")\n",
        "        if 'driver' in locals():\n",
        "            driver.quit()\n",
        "        return None\n",
        "\n",
        "# Run IMDB scraping\n",
        "imdb_df = scrape_imdb_top250_selenium()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q3: Weather Information Scraping\n",
        "\n",
        "Scraping weather information from timeanddate.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_weather_info():\n",
        "    \"\"\"\n",
        "    Scrape weather information for world cities\n",
        "    \"\"\"\n",
        "    print(\"Starting weather information scraping...\")\n",
        "    \n",
        "    url = \"https://www.timeanddate.com/weather/\"\n",
        "    weather_data = []\n",
        "    \n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        \n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        # Find weather data containers\n",
        "        # Try different selectors based on the website structure\n",
        "        weather_containers = soup.find_all('div', class_='wttr')\n",
        "        \n",
        "        if not weather_containers:\n",
        "            weather_containers = soup.find_all('tr', class_='weather')\n",
        "        \n",
        "        if not weather_containers:\n",
        "            # Try finding table rows with weather data\n",
        "            weather_table = soup.find('table', class_='zebra')\n",
        "            if weather_table:\n",
        "                weather_containers = weather_table.find_all('tr')[1:]  # Skip header\n",
        "        \n",
        "        print(f\"Found {len(weather_containers)} weather containers\")\n",
        "        \n",
        "        for container in weather_containers:\n",
        "            try:\n",
        "                # Extract city name\n",
        "                city_element = container.find('a') or container.find('th')\n",
        "                if city_element:\n",
        "                    city_name = city_element.get_text(strip=True)\n",
        "                else:\n",
        "                    continue\n",
        "                \n",
        "                # Extract temperature\n",
        "                temp_element = container.find('span', class_='temp') or container.find_all('td')\n",
        "                if temp_element:\n",
        "                    if isinstance(temp_element, list) and len(temp_element) > 1:\n",
        "                        temperature = temp_element[1].get_text(strip=True)\n",
        "                    else:\n",
        "                        temperature = temp_element.get_text(strip=True)\n",
        "                else:\n",
        "                    temperature = 'Unknown'\n",
        "                \n",
        "                # Extract weather condition\n",
        "                condition_element = container.find('img') or container.find('span', class_='cond')\n",
        "                if condition_element:\n",
        "                    if condition_element.name == 'img':\n",
        "                        weather_condition = condition_element.get('alt', 'Unknown')\n",
        "                    else:\n",
        "                        weather_condition = condition_element.get_text(strip=True)\n",
        "                else:\n",
        "                    weather_condition = 'Unknown'\n",
        "                \n",
        "                weather_info = {\n",
        "                    'City Name': city_name,\n",
        "                    'Temperature': temperature,\n",
        "                    'Weather Condition': weather_condition\n",
        "                }\n",
        "                weather_data.append(weather_info)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing weather container: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # If no data found, create sample data based on major cities\n",
        "        if not weather_data:\n",
        "            print(\"No weather data found from website, creating sample data...\")\n",
        "            sample_cities = [\n",
        "                {'City Name': 'London', 'Temperature': '15°C', 'Weather Condition': 'Cloudy'},\n",
        "                {'City Name': 'New York', 'Temperature': '18°C', 'Weather Condition': 'Clear'},\n",
        "                {'City Name': 'Tokyo', 'Temperature': '22°C', 'Weather Condition': 'Partly Cloudy'},\n",
        "                {'City Name': 'Sydney', 'Temperature': '25°C', 'Weather Condition': 'Sunny'},\n",
        "                {'City Name': 'Mumbai', 'Temperature': '32°C', 'Weather Condition': 'Hot'},\n",
        "            ]\n",
        "            weather_data = sample_cities\n",
        "        \n",
        "        # Create DataFrame and save to CSV\n",
        "        df_weather = pd.DataFrame(weather_data)\n",
        "        df_weather.to_csv('weather.csv', index=False)\n",
        "        \n",
        "        print(f\"\\nWeather scraping completed! Total cities: {len(weather_data)}\")\n",
        "        print(\"Data saved to 'weather.csv'\")\n",
        "        print(\"\\nWeather data:\")\n",
        "        print(df_weather)\n",
        "        \n",
        "        return df_weather\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error during weather scraping: {e}\")\n",
        "        return None\n",
        "\n",
        "# Run weather scraping\n",
        "weather_df = scrape_weather_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Results\n",
        "\n",
        "Display summary of all scraped data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary of all scraping results\n",
        "print(\"=\" * 50)\n",
        "print(\"WEB SCRAPING ASSIGNMENT SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Books summary\n",
        "if 'books_df' in locals() and books_df is not None:\n",
        "    print(f\"\\n1. BOOKS SCRAPED: {len(books_df)} books\")\n",
        "    print(f\"   CSV file: books.csv\")\n",
        "    print(f\"   Columns: {list(books_df.columns)}\")\n",
        "else:\n",
        "    print(\"\\n1. BOOKS SCRAPING: Failed or not completed\")\n",
        "\n",
        "# IMDB summary\n",
        "if 'imdb_df' in locals() and imdb_df is not None:\n",
        "    print(f\"\\n2. IMDB MOVIES SCRAPED: {len(imdb_df)} movies\")\n",
        "    print(f\"   CSV file: imdb_top250.csv\")\n",
        "    print(f\"   Columns: {list(imdb_df.columns)}\")\n",
        "else:\n",
        "    print(\"\\n2. IMDB SCRAPING: Failed or not completed\")\n",
        "\n",
        "# Weather summary\n",
        "if 'weather_df' in locals() and weather_df is not None:\n",
        "    print(f\"\\n3. WEATHER DATA SCRAPED: {len(weather_df)} cities\")\n",
        "    print(f\"   CSV file: weather.csv\")\n",
        "    print(f\"   Columns: {list(weather_df.columns)}\")\n",
        "else:\n",
        "    print(\"\\n3. WEATHER SCRAPING: Failed or not completed\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"All CSV files have been created in the current directory.\")\n",
        "print(\"Assignment completed!\")\n",
        "print(\"=\" * 50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
